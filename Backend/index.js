require("dotenv").config();
const express = require("express");
const bodyParser = require("body-parser");
const { OpenAI } = require("openai");

const app = express();
const port = 3000;

// Middleware
app.use(bodyParser.json());

// OpenAI Configuration
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// List of predefined topics and difficulty levels
const topics = ["JavaScript", "Python", "React", "Node.js", "CSS"];
const difficulties = ["easy", "medium", "hard"];

// Route to automatically generate a question
app.get("/generate-question", async (req, res) => {
  // Randomly select a topic and difficulty
  const randomTopic = topics[Math.floor(Math.random() * topics.length)];
  const randomDifficulty =
    difficulties[Math.floor(Math.random() * difficulties.length)];

  try {
    // Create the prompt for OpenAI
    const prompt = `Generate an interview question about ${randomTopic} with ${randomDifficulty} difficulty.`;

    // Make the OpenAI API call to generate the question
    const response = await openai.chat.completions.create({
      model: "gpt-3.5-turbo", // Use GPT-3.5 instead of GPT-4
      messages: [{ role: "user", content: prompt }],
    });


    // Log the full response for debugging
    console.log("OpenAI API Response:", response);

    // Extract the generated question from the response
    const question = response.choices[0]?.message?.content?.trim();

    // If no question is returned, return an error
    if (!question) {
      return res
        .status(500)
        .json({ error: "No question generated by OpenAI." });
    }

    // Respond with the generated question
    res.json({
      topic: randomTopic,
      difficulty: randomDifficulty,
      question: question,
    });
  } catch (error) {
    // Log detailed error information
    console.error("Error during OpenAI API call:", error);
    res
      .status(500)
      .json({ error: `Failed to generate question: ${error.message}` });
  }
});

// Start the server
app.listen(port, () => {
  console.log(`AI Interviewer backend is running at http://localhost:${port}`);
});
